{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlkO-MhBY2BE"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AdmiralHonda/ml_intro/blob/main/appendix/wiki_train.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_E6T8bl4vC",
        "outputId": "c0751162-3041-428c-ed41-8b74ebf2e881"
      },
      "outputs": [],
      "source": [
        "# 下準備\n",
        "# 形態素分析ライブラリーMeCab と 辞書(mecab-ipadic-NEologd)のインストール \n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null # mecabの利用に必要なライブラリのインストール\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null                    # gitから辞書ファイルのクローン\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1                   # クローンした辞書のインストール\n",
        "!pip install mecab-python3==0.7 > /dev/null                                                             # 0.7意外だと謎のエラーが発生して安定しないことがある\n",
        "\n",
        "# シンボリックリンクによるエラー回避\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc                                                              # 辞書の参照先にインストール先のディレクトリを追加\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"                                                    # mecabの設定ファイルに新しく辞書を追加したことを追記"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSxucG1omf6r"
      },
      "source": [
        "# **学習と推論**\n",
        "\n",
        "この章では今までの学習を生かしてアプリの根幹となる部分を構築します。  \n",
        "具体的には以下の処理を行います。  \n",
        "- 必要なデータの配置\n",
        "- 分散表現（単語をベクトル化したもの）の学習\n",
        "- 分散表現を生かした文章ベクトルの生成\n",
        "- ユーザーの入力の文章ベクトルへの変換\n",
        "- 簡単に試してみよう\n",
        "\n",
        "## **使用するデータ**\n",
        "\n",
        "### **大容量データを扱う工夫**\n",
        "今回単語の意味を学習するのに使用するデータはwikipediaの本文すべてです。  \n",
        "そのデータはテキストファイルではありますが、3.5Gもあります。  \n",
        "これだけ大きくなるとGoogleDriveでダウンロードする際にウイルスチェックができないので確認画面が表示されます。  \n",
        "そうするとプログラムから呼び出した際には、確認画面のhtmlが入力され、データ本体は読み込めなくなります。  \n",
        "#### **Colabのアップロードの遅さ**\n",
        "またこのColab環境では無料で使える代わりに、データのアップロードがとても遅く、これほどの規模のデータとなると、いったんローカルにダウンロードしてからアップロードすると何時間もかかってしまいます。  \n",
        "そのため、今起動しているcolabの仮想マシンに皆さんのGoogleDriveをマウントすることで大容量のデータのやり取りを行います。  \n",
        "正直それでも遅いですが、まだ現実的な時間で実行することができます。  \n",
        "#### **Driveへデータを配置する**\n",
        "マウント（別のマシンのディレクトリを今のマシンでも扱えるようにする）は後で説明しますが、まずは皆さんのGoogleDriveに下記のデータを配置してください。  \n",
        "まずは今のアカウントの保存容量を確認してください。  \n",
        "今回の演習では約5Gのデータをダウンロードするので、空き容量がない場合は購入かデータの削除が必要です。  \n",
        "ただ、対面でのword2vecの学習は数時間かかるので行いません。なので今回は学習済みデータを配るので、最低1.5Gくらいの空きがあれば対応できます。  \n",
        "また、日大のアカウントでは容量無限大なので日大の在校生の場合は学校のアカウントでやるといいでしょう。  \n",
        "ほかのデータと区別できるように皆さんのdriveの一番上の階層に`/ml_intro`というディレクトリを作成し、その中に下記のデータを配置してください。  \n",
        "\n",
        ">[wikiの学習データ](https://admiralhonda-share-tech.on.drv.tw/python_ml_intro/data/class_select_app/wiki_wakati.txt)  \n",
        "対面の講義ではおそらく使用しませんが、宿題や確認で学習する際に使用してください。  \n",
        "\n",
        ">[wikiから学習した学習済みデータ](https://admiralhonda-share-tech.on.drv.tw/python_ml_intro/data/class_select_app/wiki_test_vec.pt)  \n",
        "対面でプレゼンをする際は学習する時間がもったいないので既に学習済みのものを使用します。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bf7zHbKwGab",
        "outputId": "e4f3b53a-7729-433b-89ac-e8729f6d1dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOEQJTCWCXv1"
      },
      "source": [
        "## ドライブのマウント解説\n",
        "`drive.mount(\"/content/drive\")`についてですが、あなたのGoogleDriveのルートディレクトリを`/content/drive`として今の実行環境では扱うようにしています。  \n",
        "現在Colabで実行しているディレクトリは`/content`直下になるのでその下に`drive`というディレクトリを作成していることになります。  \n",
        "パソコンで見ている場合は左側にあるファルダマークをクリックしてみるとよいでしょう。  \n",
        "今の作業ディレクトリの下にあるツリーが見れます。  \n",
        "また`/content/drive`の下に`/My Drive`というディレクトリがありますが、その下にあなたGoogleDriveの中身が入っているはずです。  \n",
        "この先、先ほど紹介した通りにファイルを配置した際には`/content/drive/My Drive/ml_intro`の後にファイル名を記述すれば参照できます。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86eEqXxKY2BI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "とりあえず学習してみる\n",
        "この設定だと、２時間近くかかります。\n",
        "\"\"\"\n",
        "from gensim.models import word2vec\n",
        "\n",
        "sentences = word2vec.LineSentence(\"/content/drive/MyDrive/python_ml_intro/data/class_select_app/wiki_wakati.txt\")\n",
        "model = word2vec.Word2Vec(sentences, iter=5,size=300,sg=1,min_count=5, window=3, workers=4,hs=0)\n",
        "model.wv.save_word2vec_format(\"/content/drive/MyDrive/python_ml_intro/data/class_select_app/wiki_test_vec.pt\",binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1dfw1lJiV8P"
      },
      "source": [
        "## **word2vecの学習**  \n",
        "\n",
        "前のコードの解説をします。\n",
        "\n",
        "### **学習するためのデータ型に**  \n",
        "\n",
        "学習するために単語が事前にスペースで区切られたテキストファイルを用意しましたが、さらに改行`\\n`ごとに分割して適切な学習単位にする必要があります。  \n",
        "前処理にて１記事毎や１コンテンツ毎に改行を入れていたのはこのためです。  \n",
        "`word2vec`のメソッドである`LineSentence`でその処理を行います。  \n",
        "引数に分割するテキストファイルのパスを渡します。  \n",
        "\n",
        "### **学習**  \n",
        "\n",
        "学習する際にテキストデータを改行で区切った`sentences`を引数にとるのはもちろんのこと、その他にもいくつか指定しています。それらについて触れます。  \n",
        "- iter  \n",
        "試行回数。何回学習データで学習するかの回数です。何回問題集を解くのかだと思えばええです。\n",
        "- size  \n",
        "入力層の列数。\n",
        "- sg  \n",
        "cbowかskip-gramかの選択。1か0で指定する。\n",
        "- min_count  \n",
        "学習データ中にあまり頻出しない単語を排除するために、出現回数が`min_count`以内であったら学習対象から除外するという設定。不必要な学習単語数の増加は重み（行列）の行数が増えるので計算時間が増えます。また、最終的にデータの量が多くなりアプリで使用する際に読み込み時間が増加する懸念があります。  私の実装ではコンテナを使用することが前提なので数秒の差は大きく見ています。\n",
        "- window  \n",
        "周辺の単語をどれだけ考慮するかの数。windowサイズです。\n",
        "- workers  \n",
        "実行するプロセスの数です。環境の論理cpu数を指定するとよいでしょう\n",
        "- hs  \n",
        "損失計算の際に階層的ソフトマックスかネガティブサンプリングを選ぶかの指定。1か0で指定。\n",
        "\n",
        "### **保存**\n",
        "\n",
        "最後の一文では入力層の部分のみを取り出して保存しています。  \n",
        "`Word2Vec`クラスのパラメータである`wv`が入力層に当たります。  \n",
        "保存するメソッドの`binary=True`はテキストでなくバイナリで保存するように指定しています。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r4H7ajdVY2BI"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wiki_model = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/python_ml_intro/data/class_select_app/wiki_word_vec.pt\",binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QL5xc0A1pdy"
      },
      "source": [
        "## 学習データの読み込み\n",
        "\n",
        "前述したコードで.pt(区別のために適当につけた拡張子)とついたファイルを単語の分散表現として保存しました。  \n",
        "今度はそれを`KeyVectors`として読み込みます。学習はできませんが、単語間の**意味**的な類似度や特定の単語のベクトルを抜き出すことができます。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExF7Mblg4XgJ"
      },
      "source": [
        "## コンテンツのベクトル化\n",
        "\n",
        "これ以降の処理ではwikipediaから学んだ単語の意味を用いてコンテンツの意味を含んだベクトルを生成します。  \n",
        "\n",
        "今回はサンプルとして授業の検索アプリを作成します。  \n",
        "少ないですが、20件の講義データをcsv形式のファイルといて用意しています。  \n",
        "これを読み込み、各講義データごとに所属する単語ベクトルの平均をとり、文章ベクトルを生成します。  \n",
        "\n",
        "### pandas\n",
        "\n",
        "pythonのライブラリで表形式のデータを処理するのに非常に便利です。  \n",
        "この３年生向けのイントロでは触れませんが、データからより学習するために不必要な記号を削除したり改行文字を取り払うなどの前処理をするときなどにも役に立ちます。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "Fff_zTQ5l2o8",
        "outputId": "22da3192-d99a-4c91-942f-c9efeffb1dde"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "classroom_info = pd.read_csv(\"https://admiralhonda-share-tech.on.drv.tw/python_ml_intro/data/class_select_app/class_info.csv\")  # データの読み込み\n",
        "classroom_info.head(10)                                                                                                           # 先頭４個分のデータを確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmfhbmMMl2o8",
        "outputId": "0af86db6-4163-42e3-9468-d00d320d441d"
      },
      "outputs": [],
      "source": [
        "classroom_info_dict = classroom_info.to_dict(orient='record')                                                                   # 読み込んだデータを辞書型に変換\n",
        "print(classroom_info_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s-0ZiougqEMA"
      },
      "outputs": [],
      "source": [
        "# Mecabのセッティング\n",
        "import MeCab\n",
        "\n",
        "tokenizer = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd -Owakati\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6d4fE4GrZLg"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.parse(\"アルゴリズム\")[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lJ3bmmvSl2o9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ここでは各講義の\n",
        "・授業目的\n",
        "・教育目標\n",
        "・概要\n",
        "の三つの文章を連結し、mecabによって\n",
        "単語の集合に分割してから各単語ベクトルの平均を求めています。\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "class_content = []                                # コンテンツのベクトルを格納するリスト\n",
        "\n",
        "for classroom in classroom_info_dict:             # classroom_info_dictには各授業の概要や教科名などが辞書型になったものがリストとして格納されている。\n",
        "  tmp = \"\"\n",
        "  tmp += classroom[\"授業目的\"]\n",
        "  tmp += classroom[\"教育目標\"]\n",
        "  tmp += classroom[\"概要\"]\n",
        "\n",
        "  sum = np.zeros(300)                             # 授業の文章ベクトルを格納する要素を0とした要素数300ノベクトルを初期化。\n",
        "  words = tokenizer.parse(tmp)[:-1].split(\" \")    # 集約した文章を単語のリストに分割。mecabで分割した際には単語間にスペースが入った文字列として出力。最後の改行は邪魔なので考慮していない。\n",
        "  recg_word_num = 0                               # 文章内で認識できた単語の数を数える\n",
        "  for word in words:\n",
        "    try:\n",
        "      sum += wiki_model[word]\n",
        "      recg_word_num += 1\n",
        "    except KeyError:                              # 学習していない単語の場合は考慮しない\n",
        "      pass\n",
        "  if recg_word_num == 0:                          # もし学習済みの単語がない場合はランダムなベクトルを割り当てる\n",
        "    class_content.append(np.random(300))\n",
        "  else:\n",
        "    class_content.append(sum/recg_word_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4tqj2Bm93hsY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class_content = np.array(class_content)                                                                   # 保存のためにコンテンツベクトルを集約したものをnumpy配列とする\n",
        "np.save(\"/content/drive/MyDrive/python_ml_intro/data/class_select_app/content_vec.npy\",class_content)\n",
        "\n",
        "output_content_info = json.dumps(classroom_info_dict,ensure_ascii=False,indent=2)                         # ユーザーに表示するデータとして保存。コンテンツベクトルと添え字を合わせる\n",
        "with open(\"/content/drive/MyDrive/python_ml_intro/data/class_select_app/content_info_dict.json\",\"w\") as f:\n",
        "  f.write(output_content_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rol11AoUCYH4"
      },
      "source": [
        "## **検索してみよう**\n",
        "せっかくなので検索して今までの成果を確認してみましょう。  \n",
        "文章を入力してそれに近い授業を見つけるのです。\n",
        "\n",
        "### **コンテンツと比較してみよう**\n",
        "\n",
        "いろいろ準備してきましたが、まだ検索はできません。ユーザーの入力はコンテンツのベクトル化と同じやり方で実装できますが、肝心の比較する部分に関して紹介していません。もう少し頑張りましょう。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CMp33mi5ZSsu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ユーザーの入力をベクトルとして返す関数の定義\n",
        "後でwebアプリ化する際に使用します。\n",
        "\"\"\"\n",
        "\n",
        "def user_input(query: str,m: MeCab.Tagger,wv:KeyedVectors) -> np.ndarray:\n",
        "  sum = np.zeros(300)                               # 授業の文章ベクトルを格納する要素を0とした要素数300ノベクトルを初期化。\n",
        "  words = tokenizer.parse(query)[:-1].split(\" \")    # 集約した文章を単語のリストに分割。mecabで分割した際には単語間にスペースが入った文字列として出力。最後の改行は邪魔なので考慮していない。\n",
        "  recg_word_num = 0                                 # 文章内で認識できた単語の数を数える\n",
        "  for word in words:\n",
        "    try:\n",
        "      sum += wv[word]\n",
        "      recg_word_num += 1\n",
        "    except KeyError:                                # 学習していない単語の場合は考慮しない\n",
        "      pass\n",
        "  if recg_word_num == 0:                            # もし学習済みの単語がない場合はランダムなベクトルを割り当てる\n",
        "    return np.random(300)\n",
        "  else:\n",
        "    return sum / recg_word_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu1LHuqpb0fa"
      },
      "source": [
        "## コンテンツとの比較\n",
        "\n",
        "コンテンツとの比較とはベクトル同士の比較です。  \n",
        "今回はコサイン尺度を紹介します。その他にもユークリッド距離やピアソン相関などがあります。   \n",
        "２つのベクトルΑ,Βがあった時の類似度Tは以下の式で算出されます。  \n",
        "\n",
        "$$Τ = \\frac{Α·Β}{|Α|·|Β|}$$  \n",
        "\n",
        "高校のcosineを座標から求めた時を思い出しましょう。  \n",
        "ベクトルを２次元座標で考えたときに同じであればそれぞれの点と点の角度は0になり(点と点との間に差がない)、まったく違うなら90°,180°になります。全く違うときにそのベクトル同士は直行するといいましたね。その時cosineで0°は１、90°は0になりましたね。0から1までの数字をとるのでこれに100をかければ何％一致したのかという解釈もできます。これを今回は比較する際の計算式とします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nFaXs3J8kGB8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ユーザーの入力（ベクトル）との比較を行う関数\n",
        "ユーザーの入力と最も類似度が高いコンテンツの添え字を返す\n",
        "\"\"\"\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def culculate_sim(query :np.ndarray ,content_vec :np.ndarray) -> int:\n",
        "  sim_rate = cosine_similarity([query],content_vec)\n",
        "  return np.argmax(sim_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shRLxMjPl6qF",
        "outputId": "d40d49b3-803a-41b9-9184-1211eb6aa73d"
      },
      "outputs": [],
      "source": [
        "user_query = \"キャリアプラン\"\n",
        "user_vec = user_input(user_query,tokenizer,wiki_model)\n",
        "print(classroom_info_dict[culculate_sim(user_vec,class_content)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mllBtt7sbLd"
      },
      "source": [
        "## 検索結果に満足できたかな？\n",
        "\n",
        "やったね。アプリ化するときの基礎が出来上がったZOY  \n",
        "でも検索結果に満足できたかな？予期しない答えはなかったですか？  \n",
        "\n",
        "### すべてを考慮してはならない\n",
        "\n",
        "何がいけなかったのか。学習データがいけないのでしょうのか（実際辞書であるwikiから学習したモデルに、話し言葉を突っ込むことに違和感はあります）  \n",
        "簡単な解決法としては考慮する単語を減らすことです。文章を読むときに全てにしっかり目を通すでしょうか？  \n",
        "実際は特徴のある単語に目をつけてその単語間のつながりを見ているとも言えます。  \n",
        "例を挙げると「昨日 、 友達 と もつ焼き の 店 に 行 った 。」という文章があった時に「友達」や「行 った」という単語はどの話し言葉にも文章にも表れますが、「もつ焼き」という単語はどの文書にも出てくるとは言えないでしょう。なのでこの文章は「もつ焼き」を中心として成り立っているといえます。  \n",
        "特に今回は文章の特徴を際立たせる必要があるので、文章内で特徴のある単語ベクトルが文章ベクトルに大きく寄与していることが望ましいです。  \n",
        "以下では文章内の単語がどれだけ重要なのか、そしてどれくらい重要なのかを数値で算出するTF-IDFという手法を紹介します。　　\n",
        "\n",
        "### TF-IDF\n",
        "\n",
        "この手法では  \n",
        "- 単語がその文章内でどれだけ出現しているか\n",
        "- 学習するデータ内でどれだけその単語が出現しているか\n",
        "で判断します。　　\n",
        "\n",
        "#### TF(Term Frequency)\n",
        "\n",
        "文章内でどれだけその単語が出現しているかを算出する式です。\n",
        "対象の単語をt、単語が所属する文章をdとすると:\n",
        "$$TF(t,d) = \\frac{num(t)}{\\Sigma  num(t') (t' \\in d)}$$\n",
        "となります。文章に含まれる単語のうち、どれだけその単語が占めているかを示しています。  \n",
        "\n",
        "#### IDF(Inverse Document Frequency)\n",
        "\n",
        "学習データ内でどれだけその単語が出現しているかを示します。  \n",
        "TFと違ってこちらは希少であるほうが良いので出現回数が小さいほど数値が大きくなるような式になっています。  \n",
        "対象の単語をt、単語が所属する文章数をdとし、全文章数をnすると:\n",
        "$$IDF = \\log \\frac{n}{d(t \\in d)} $$\n",
        "となります。対数をとっているのは文章数が膨大になっても計算可能な範囲に留めておくためです。\n",
        "\n",
        "以上２つの指標を掛けたものが単語がどれだけ重要かを示し指標であるTF-IDFです。  \n",
        "\n",
        "$$TFIDF = TF \\cdot IDF$$\n",
        "\n",
        "これで単語の重要度を数値で表すことができました。  \n",
        "今度はこれを使って文章ベクトルを作る際に各単語ベクトルに重みとしてかけることで接続詞や助詞などのよく出る単語を低くし、固有名詞などの特徴ある単語を際立たせてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTZzV2X1Dx2G"
      },
      "outputs": [],
      "source": [
        "# tf-idfを計算するコードだが、メモリが64G以上ない場合はやめておきましょう\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "dic = corpora.Dictionary(corpus)\n",
        "dic.save_as_text(\"./wiki_dic.dic\",sort_by_word=True)\n",
        "input_corpus = list(map(dic.doc2bow,corpus))                                                                                            # corpusは２次元配列で、各文章を単語のリストとして格納\n",
        "test_model = TfidfModel(input_corpus)\n",
        "test_model.save(\"./wiki_tfidf.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QCIUawjdkS6e"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "dic = corpora.Dictionary.load_from_text(\"https://admiralhonda-share-tech.on.drv.tw/python_ml_intro/data/class_select_app/wiki_dic.dic\")\n",
        "test_model = TfidfModel().load(\"https://admiralhonda-share-tech.on.drv.tw/python_ml_intro/data/class_select_app/wiki_tfidf.model\")\n",
        "\n",
        "\"\"\"\n",
        "ここでは各文章毎に属している単語がどれだけ重要なものであるかを重みづけしていきます。\n",
        "\"\"\"\n",
        "\n",
        "tmp_content_split = [ tokenizer.parse(classroom[\"授業目的\"] + classroom[\"教育目標\"] + classroom[\"概要\"]).replace(\"\\n\",\"\").split(\" \") for classroom in classroom_info_dict]\n",
        "input_corpus = list(map(dic.doc2bow,tmp_content_split))  \n",
        "content_tfidf = test_model[input_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uF7USkSJLP3a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ここでは各講義の\n",
        "・授業目的\n",
        "・教育目標\n",
        "・概要\n",
        "の三つの文章を連結し、mecabによって\n",
        "単語の集合に分割してから各単語ベクトルにtfidfで得た重みを掛けたベクトルの加重平均を求めています。\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "class_content = []                                # コンテンツのベクトルを格納するリスト\n",
        "\n",
        "for content in content_tfidf:\n",
        "\n",
        "  sum = np.zeros(300)                             # 授業の文章ベクトルを格納する要素を0とした要素数300ノベクトルを初期化。\n",
        "  recg_word_num = 0                               # 文章内で認識できた単語だけtfidfの重みを加算する\n",
        "  for word in content:\n",
        "    try:\n",
        "      sum += wiki_model[dic[word[0]]] * word[1]\n",
        "      recg_word_num += word[1]\n",
        "    except KeyError:                              # 学習していない単語の場合は考慮しない\n",
        "      pass\n",
        "      \n",
        "  if recg_word_num == 0:                          # もし学習済みの単語がない場合はランダムなベクトルを割り当てる\n",
        "    class_content.append(np.random(300))\n",
        "  else:\n",
        "    class_content.append(sum/recg_word_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azO2HS61nAi9",
        "outputId": "3f0f1aec-1006-4358-beff-7471c9b1b574"
      },
      "outputs": [],
      "source": [
        "user_query = \"キャリアプラン\"\n",
        "user_vec = user_input(user_query,tokenizer,wiki_model)\n",
        "print(classroom_info_dict[culculate_sim(user_vec,class_content)])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9518b5d8b05613f33dd9892199209c6ea159e58ef30c24ef8a970c59421088c5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
